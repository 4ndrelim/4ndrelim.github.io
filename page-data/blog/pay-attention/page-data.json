{"componentChunkName":"component---src-templates-blog-post-jsx","path":"/blog/pay-attention/","result":{"data":{"site":{"siteMetadata":{"name":"Andre Lin","title":"Andre Lin | My Simple Site","description":"An aspring Machine Learning engineer hoping to integrate ML deployment into software development.","about":"Hey! My name is Andre. I'm a third year undergraduate studying Mathematics & Computer Science at the National University of Singapore (NUS).\n      I like reading about research on AI and advancements in ML, and spend a fair portion of my time doing so. ML is a field where I can put what i've learnt in brutal math courses (ğ˜¢ğ˜µ ğ˜­ğ˜¦ğ˜¢ğ˜´ğ˜µ ğ˜´ğ˜°ğ˜®ğ˜¦ ğ˜°ğ˜§ ğ˜ªğ˜µ.. *_*) to use. I believe there's plenty of exciting research done in the ML landscape, and more can certainly deployed in practice.\n      Deep learning, Natural Language Processing, AI decision making - are some sub-fields that excite me. But i've some exposure in the basics of other fields - Databases, Networks, Operating Systems, Parallel & Distributed systems to name a few. I hope to hone my knowledge in my remaining semesters and become a proficient ML engineer able to bridge the gap between software development and ML deployment.\n      Beyond academia, I am an avid climber and belong to the school's Mountaineering club. Together with a couple of ğ˜¸ğ˜°ğ˜¯ğ˜¥ğ˜¦ğ˜³ğ˜§ğ˜¶ğ˜­, ğ˜§ğ˜¶ğ˜¯-ğ˜­ğ˜°ğ˜·ğ˜ªğ˜¯ğ˜¨, ğ˜¢ğ˜¯ğ˜¥ ğ˜¤ğ˜¶ğ˜¤ğ˜¬ğ˜°ğ˜° friends, we scaled a mountain in the Himalayas and it was, well, simply fantastic.","author":"","email":"andre_lin@u.nus.edu","github":"https://github.com/4ndrelim","linkedin":"https://www.linkedin.com/in/andre-linhk/"}},"markdownRemark":{"id":"73ba7fdb-15df-5c82-9081-829a91d9c102","excerpt":"Edit: At the time of writing, Grant from 3Blue1Brown released a marvelous video offering a high-level intuition behind the transformer architecture.\n\n Hisâ€¦","html":"<p><span style=\"font-size:10px;\">Edit: At the time of writing, Grant from 3Blue1Brown released a marvelous video offering a high-level intuition behind the transformer architecture.</span>\n<br/>\n<span style=\"font-size:10px;\">His videos are without a doubt, top-notch. But if youâ€™re more inclined for a short read than watch, this piece might just be for you.</span></p>\n<p>-insert scary transformer pic-</p>\n<h2>The Last Decade</h2>\n<p>Rcurrent Neural Networks (RNNs) were among the first architectures to process sequential data. And until 2017, RNNs remained as the primary neural network architecture, with Long Short-Term Memory (LSTM) units and Grated Recurrent Units (GRUs) - which can be thought of as a simpler version of LSTMs - being the most prominent and effective variants. In the present, LSTMs are becoming increasing obsolete, particularly in many areas of Natural Language Processing (NLP).</p>\n<details>\n    <summary>Applications for LSTMs</summary>\n    <span style=\"font-size:14px;\">\n        Calling LSTMs 'obsolete' might be slightly too harsh. There remain use cases for LSTMs:\n        <ul>\n            <li><strong>Resource Constraints:</strong> LSTMs are more suitable for environments with limited computational resources, such as mobile devices.</li>\n            <li><strong>Real-time Processing:</strong> Ideal for applications where data is processed in real-time, such as streaming data.</li>\n            <li><strong>Small Data Scenarios:</strong> Performance degrades with increasing in amount of training data; after all, LSTM mitigates, not eliminate, exploding/vanishing gradient descent problem.</li>\n            <li><strong>Specific Types of Sequential Tasks:</strong> Such as time-series forecasting where long-term historical context is valuable.</li>\n        </ul>\n    </span>\n</details>\n<details>\n    <summary>Understanding the LSTM Unit</summary>\n    <span style=\"font-size:14px;\">\n        Understanding LSTM isn't the main focus of this piece. But below outlines my brief intuition behind the LSTM unit\n    </span>\n</details>\n<p><br><br/>\nBut there is merit for aspiring machine learners to study (or at least grasp the idea) the evolution of architectures starting from RNNs. Understanding their limitations sheds light on the inspiration behind the next development. RNNs sought to retain past information to influence subsequent outputs. They were simplistic in their design, which made them popular but also inadequate. It suffers greatly from exploding/vanishing gradient descent problem, which meant it is ultimately limited in capturing long-term dependencies.</p>\n<p>LSTMs came into the picture and the key innovation in LSTMs that helps manage gradient issues is their gated architecture, better regulating the flow of information through the network. However, in practice, LSTM units are still chained in succession, and will struggle with extremely long sequences. Processing data sequentially (each step must wait for the previous step to complete) inherently limits the parallelization within the training process. And because it is uni-direcitonal (information only flows one way), it is perhaps not suited in capturing nuances in natural language due to its limited ability in gathering context from both past and future simultaneously.</p>\n<p>Thus, the advent of Transformers.</p>\n<h2>The Transformer</h2>\n<details>\n  <summary><b>A Fundamental Appreciation</b></summary>\n    <span style=\"font-size:14px;\">\n      <p>Before going any further, it would be greatly beneficial to have acquired an understanding of the vanilla (dense) neural networks. Appreciate that ultimately, neural networks devolve to <b>just a bunch of weights and biases</b>. These are trainable parameters encapsulated in what is commonly referred to as a 'black box' - takes in some input and (hopefully) churn the desired output. <b>The hope becomes tangible if the optimal values of weights of biases are found</b>. And how these values are set, depends on the training process.</p>\n      <p>Of course, there are more transformations applied, such as activation functions to introduce non-linearity and batch normalization for stability. You can think of these as fancy accessories that can be swapped among various kinds. And I suppose it does make some sense why they are desired, after all, if all we do is to linearly apply weights and biases, the accumulation of all operations ultimately remains linear no matter how many layers of weights and biases. <br/>\n      <b>What hope would we have that this could capture intricate relationships with no clear separation</b>?</p>\n      *-insert vanilla NN with activation fn-*\n      <p><b>Watch this <a href=\"https://www.youtube.com/watch?v=aircAruvnKk\">series</a> by 3Blue1Brown on Deep Learning if you're foreign to all these</b>! Episodes 1-3 offer an enlightening and intuitive perspective of Deep Learning. Episode 4 goes through the calculus behind Gradient Descent, the technique behind the training phase, and might be slightly heavy. If you're math-averse (though frankly, it's the notation that confuses people rather than the underlying concepts), feel free to skip it. The first 3 episodes should sufficiently and satisfyingly equip you with a solid grasp.</p>\n      <p>A short aside on the term 'black box'. Not entirely a big fan of the term though I empathise its common usage. Indeed, the wall of numbers in the layers of a neural network can seem intimidating, and we have no robust intuitive explanation why the input magically transforms into the output. But the key insight is to realise that with a suitable loss function, we can, with Gradient Descent, guide the model to tune its weights and biases based on how similar it is to the desired output. <b>How do we know that Gradient Descent will correctly tune the weights?</b> \n      <br/>\n      Well, this is where Mathematics enter. We can, mathematically show that <b>Gradient Descent always converges to some local minima</b>. Some people may lament that a local minimum isn't the global minimum. <b>But one should question, is the global minimum even desired..?</b> Recall that optimizing the objective function (used in training) does not always translate to the lowest cost in actual production / testing. More often than not, there exists a 'good enough' range of values for the weights to achieve satisfactory performance on the actual cost function. So perhaps, the local minimum found might just fall within this good region, and in some cases, might actually perform better on actual live data than with the weights found from the global minimum.</p>\n      *-insert objective fn vs cost fn-*\n      <p>One last thing, most people confuse the relationship between Backpropagation and Gradient Descent. Gradient Descent leverages Backpropagation to efficiently compute derivatives (gradients) of the loss function with respect to each tunable weights in the network. Sure, you can opt for a different technique to compute gradients, but it likely won't be as efficient as Backpropagation (which only require 2 passes!).</p>\n    </span>\n</details>\n<p><br><br/>\nThe Transformer was introduced in the paper titled <a href=\"https://arxiv.org/abs/1706.03762\">â€œAttention Is All You Needâ€</a> by a team of researchers from Google Brain. This groundbreaking paper revolutionized the field of Natural Language Processing, giving rise to various Large Language Models (LLMs) with vast linguistic capabilities - powerful enough to convince some people that a dystopian era of sentient (lol..) robots is looming.</p>\n<p>Our beloved ChatGPT too, leverages a variant of the original transformer architecture to answer at our beck and call, mostly silly questions.</p>\n<details>\n  <summary>Anyway, before we continue our exploration of the transformer architecture from bottom-up, i'd like to try to convince (if you aren't already) why it shouldn't be too surprising that an inanimate object can sound so much like a human:</summary>\n  <span style=\"font-size:14px;\">\n    <i>The Gentlemen Bastards</i> is one of my favourite medieval-fantasy series. It incorporated humour so well in the midst of chaos and danger. <i>The First Law</i> is also another series I greatly enjoyed, the grim fantasy setting and pragmatic characters was something I found oddly amusing. The authors have a way with words, they brought the characters to life and got me real hooked on them. But it's not like the authors actually walked into my study, sat next to me, and read their work aloud with such intensity to convey every intended nuance. All I did was read a chunk of text.\n  </span>\n</details>\n<p><br></br>\nAnd turns out, the 2017 paper has cracked the code behind human speech. We can train models to chain the right words, with the right punctuation, the appropriate pauses, and whatnot, altogether to emulate human speech.</p>\n<p>Letâ€™s explore how it does it :)</p>\n<h3>Word Embeddings</h3>\n<p>Machines generally only handle numeric input. So, words must first be converted to some numeric form. A vector of numbers is a natural choice. One common method used in the past is to represent words through <a href=\"https://www.youtube.com/watch?v=G2iVj7WKDFk\">one-hot encoding (OHE)</a>. Already, you can probably sense this isnâ€™t a good choice. For one, the length of the vector has to be the size of your vocabulary, which clearly does not scale well.</p>\n<details>\n  <summary>Why?</summary>\n  <span style=\"font-size:14px;\">\n    Adding 1 more unique word to the vocabulary means adding 1 more dimension to the representation!\n  </span>\n</details>\nOne-hot encoded vectors are extremely sparse (each word exists in its own dimension). And it doesn't quite capture semantic relationship between words that are 'closer' in meaning, for instance, male and female vs animal. \n<p>Thus, the need for word embeddings. Word embeddings are still vectors of numbers, but now, distance between the vectors matter. <b>Think of these vectors as giving coordinates in some high dimensional space (albeit not as high as OHE) where words with similar meanings tend to be closer to each other in that space</b>. This might sound very intuitive and almost trivial, but you might be surprised to learn that it wasnâ€™t until the mid-2010s that word embeddings received widespread adoption.</p>\n<p>The implication is that the model can now learn patterns or relationship among words apply it elsewhere. For instance, if the model learns the vector difference (helpful for visualisation, but god knows whether the model learns more fanciful operations instead) between the representations of <em>Man</em> and <em>Woman</em>, it could apply and learn <em>King</em> as to <em>Queen</em>. <br/>\nAs another example, consider <em>Pasta</em> as to <em>Italy</em>, and if I ask <em>Ramen</em> as to <em>??</em>, you would most likely suggest <em>Japan</em>. This vector space representation is precisely what we desire to meaningfully capture the words.</p>\n<details>\n  <summary>How?</summary>\n  <span style=\"font-size:14px;\">\n    <p>There are popular and powerful techniques such as Word2Vec and GloVe that can capture word embeddings very well. If you're mostly dealing with normal english (and probably other languages) vocabulary, existing pre-trained GloVe and Word2Vec models should already be highly effective. After all, we don't expect the English Dictionary to change too often.</p>\n    <p>If you're interested, Word2Vec employs a shallow neural network (SURPRISE SURPRISE..) architecture to learn word embeddings. GloVe obtained embeddings differently by calculating global co-occurrence matrix. Yes.. fancy terms, feel free to read up.</p>\n  </span>\n</details>\n<h3>Positional Encodings</h3>\n<h3>Tokenization</h3>\n<p>Iâ€™ve been casually assuming the model knows how to parse a sentence into individual words, and very naturally such that the individual â€˜wordsâ€™ align with the words in our english language vocabulary. This isnâ€™t always the case. A more appropriate terminology would be â€˜tokensâ€™. A token could be a word or little pieces of words that are commonly used. This offers greater flexibility. For instance, the model might find it more helpful to have <em>â€˜-ingâ€™</em> as a token rather than have to separately capture <em>â€˜eatingâ€™</em> and <em>â€˜playingâ€™</em> on top of <em>â€˜eatâ€™</em> and <em>â€˜playâ€™</em>.</p>\n<p>In practice, we often use some popular tokenizer from a Library (e.g. NLTK) to parse the corpus of text. Embeddings are then computed from these tokens found. The collection of all tokens is your dictionary (aka vocabulary).</p>\n<h3>The Secret Sauce: Pay Attention!</h3>\n<h3>Trusty-old Softmax</h3>\n<h2>Heart of the Transformer: Self-Attention</h2>\n<h2>Different Architectures</h2>\n<h3>Encoder-Decoder</h3>\n<h3>Encoder-Only</h3>\n<h3>Decoder-Only</h3>\n<h2>Heck, they are all the same!</h2>","frontmatter":{"title":"Nothing Fancy, Pay Attention","date":"December 09, 2023","description":"Attention Mechanism behind LLMs - One of the more enlightening moments from this hectic semester"}}},"pageContext":{"slug":"/blog/pay-attention/","previous":{"fields":{"slug":"/blog/vanished/"},"frontmatter":{"title":"Monthly Diaries Didn't Last.. Oops"}},"next":{"fields":{"slug":"/blog/seeds-of-change/"},"frontmatter":{"title":"Seeds of Change (1)"}}}},"staticQueryHashes":["63159454"]}