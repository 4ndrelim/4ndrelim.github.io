<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.a0dc5e2ca3094004d7aa.css">/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button{-webkit-appearance:button}button::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}details{display:block}summary{display:list-item}[hidden],template{display:none}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}button{background-color:transparent;background-image:none}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}fieldset,ol,ul{margin:0;padding:0}ol,ul{list-style:none}html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;line-height:1.5}*,:after,:before{box-sizing:border-box;border:0 solid #e2e8f0}hr{border-top-width:1px}img{border-style:solid}textarea{resize:vertical}input::placeholder,textarea::placeholder{color:#a0aec0}button{cursor:pointer}table{border-collapse:collapse}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit}code,kbd,pre,samp{font-family:Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}.blog-content{--text-opacity:1;color:#718096;color:rgba(113,128,150,var(--text-opacity));max-width:768px}.blog-content a{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity));text-decoration:underline}.blog-content p{margin-bottom:1.5rem}.blog-content ol,.blog-content ul{margin-bottom:1.5rem;margin-left:2.5rem}.blog-content ul li{list-style-type:disc}.blog-content ol li{list-style-type:decimal}.blog-content h1{font-size:2.25rem}.blog-content h1,.blog-content h2{--text-opacity:1;color:#1a202c;color:rgba(26,32,44,var(--text-opacity));font-weight:700;margin-top:.75rem;margin-bottom:.75rem}.blog-content h2{font-size:1.5rem}.blog-content h3{font-size:1.25rem}.blog-content h3,.blog-content h4,.blog-content h5{--text-opacity:1;color:#1a202c;color:rgba(26,32,44,var(--text-opacity));font-weight:700;margin-top:.75rem;margin-bottom:.75rem}.blog-content h5{font-size:.875rem}.blog-content h6{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity));font-size:.75rem;font-weight:700;margin-top:.75rem;margin-bottom:.75rem}.blog-content blockquote{padding-left:1.25rem;border-left-width:4px}.blog-content hr{margin-top:1.5rem;margin-bottom:1.5rem}.rounded-full{border-radius:9999px}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.table{display:table}.hidden{display:none}.flex-none{flex:none}.font-light{font-weight:300}.font-semibold{font-weight:600}.font-bold{font-weight:700}.text-xs{font-size:.75rem}.text-sm{font-size:.875rem}.text-lg{font-size:1.125rem}.text-4xl{font-size:2.25rem}.text-5xl{font-size:3rem}.leading-tight{line-height:1.25}.leading-normal{line-height:1.5}.list-none{list-style-type:none}.mt-6{margin-top:1.5rem}.mb-6{margin-bottom:1.5rem}.mt-16{margin-top:4rem}.max-w-screen-xl{max-width:1280px}.p-8{padding:2rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.pb-1{padding-bottom:.25rem}.pr-4{padding-right:1rem}.pt-6{padding-top:1.5rem}.pb-6{padding-bottom:1.5rem}.pt-12{padding-top:3rem}.fixed{position:fixed}.relative{position:relative}.right-0{right:0}.text-gray-200{--text-opacity:1;color:#edf2f7;color:rgba(237,242,247,var(--text-opacity))}.text-gray-500{--text-opacity:1;color:#a0aec0;color:rgba(160,174,192,var(--text-opacity))}.text-gray-600{--text-opacity:1;color:#718096;color:rgba(113,128,150,var(--text-opacity))}.text-gray-900{--text-opacity:1;color:#1a202c;color:rgba(26,32,44,var(--text-opacity))}.hover\:text-black:hover{--text-opacity:1;color:#000;color:rgba(0,0,0,var(--text-opacity))}.uppercase{text-transform:uppercase}.hover\:underline:hover,.underline{text-decoration:underline}.tracking-wider{letter-spacing:.05em}.tracking-widest{letter-spacing:.1em}.w-full{width:100%}.z-0{z-index:0}.transform{--transform-translate-x:0;--transform-translate-y:0;--transform-rotate:0;--transform-skew-x:0;--transform-skew-y:0;--transform-scale-x:1;--transform-scale-y:1;transform:translateX(var(--transform-translate-x)) translateY(var(--transform-translate-y)) rotate(var(--transform-rotate)) skewX(var(--transform-skew-x)) skewY(var(--transform-skew-y)) scaleX(var(--transform-scale-x)) scaleY(var(--transform-scale-y))}.hover\:scale-105:hover{--transform-scale-x:1.05;--transform-scale-y:1.05}.transition-all{transition-property:all}.duration-150{transition-duration:.15s}@keyframes spin{to{transform:rotate(1turn)}}@keyframes ping{75%,to{transform:scale(2);opacity:0}}@keyframes pulse{50%{opacity:.5}}@keyframes bounce{0%,to{transform:translateY(-25%);animation-timing-function:cubic-bezier(.8,0,1,1)}50%{transform:none;animation-timing-function:cubic-bezier(0,0,.2,1)}}.max-w-150{max-width:150px}.top-5{top:5%}code[class*=language-],pre[class*=language-]{color:#c5c8c6;text-shadow:0 1px rgba(0,0,0,.3);font-family:Inconsolata,Monaco,Consolas,Courier New,Courier,monospace;font-size:.75rem;direction:ltr;margin-bottom:1.5rem;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto!important;border-radius:.3em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#1d1f21}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#7c7c7c}.token.punctuation{color:#c5c8c6}.namespace{opacity:.7}.token.keyword,.token.property,.token.tag{color:#96cbfe}.token.class-name{color:#ffffb6;text-decoration:underline}.token.boolean,.token.constant{color:#9c9}.token.deleted,.token.symbol{color:#f92672}.token.number{color:#ff73fd}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#a8ff60}.token.variable{color:#c6c5fe}.token.operator{color:#ededed}.token.entity{color:#ffffb6;cursor:help}.token.url{color:#96cbfe}.language-css .token.string,.style .token.string{color:#87c38a}.token.atrule,.token.attr-value{color:#f9ee98}.token.function{color:#dad085}.token.regex{color:#e9c062}.token.important{color:#fd971f}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.language-text{background:#edf2f7!important;border-radius:0!important;color:#4a5568!important;font-size:12px!important;padding:2px 5px!important;text-shadow:none!important}@media (min-width:768px){.md\:flex{display:flex}.md\:flex-1{flex:1 1 0%}.md\:p-0{padding:0}.md\:pt-1{padding-top:.25rem}.md\:pl-20{padding-left:5rem}.md\:w-full{width:100%}.md\:max-w-150{max-width:150px}}@media (min-width:1280px){.xl\:block{display:block}}@media (min-width:480px){.xs\:p-24{padding:6rem}}</style><meta name="generator" content="Gatsby 2.24.67"/><title data-react-helmet="true">Nothing Fancy, Pay Attention | Andre Lin | My Simple Site</title><meta data-react-helmet="true" name="description" content="An aspring Machine Learning engineer hoping to integrate ML deployment into software development."/><meta data-react-helmet="true" property="og:title" content="Nothing Fancy, Pay Attention | Andre Lin | My Simple Site"/><meta data-react-helmet="true" property="og:description" content="An aspring Machine Learning engineer hoping to integrate ML deployment into software development."/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" property="twitter:card" content="summary"/><meta data-react-helmet="true" property="twitter:creator" content=""/><meta data-react-helmet="true" property="twitter:title" content="Nothing Fancy, Pay Attention | Andre Lin | My Simple Site"/><meta data-react-helmet="true" property="twitter:description" content="An aspring Machine Learning engineer hoping to integrate ML deployment into software development."/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><link rel="preconnect" href="https://www.google-analytics.com"/><link rel="dns-prefetch" href="https://www.google-analytics.com"/><link rel="icon" href="/favicon-32x32.png?v=5c504cfe621df9b7c40af602f0f51573" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><meta name="theme-color" content="#663399"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=5c504cfe621df9b7c40af602f0f51573"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=5c504cfe621df9b7c40af602f0f51573"/><link as="script" rel="preload" href="/webpack-runtime-2d432607a065c60c4715.js"/><link as="script" rel="preload" href="/styles-755093da0c07f4b49226.js"/><link as="script" rel="preload" href="/framework-e2d419ac45d8ae41957a.js"/><link as="script" rel="preload" href="/app-0a9070be329fd8f91afd.js"/><link as="script" rel="preload" href="/commons-460f29ae88868401563f.js"/><link as="script" rel="preload" href="/500c2994e08cbb1cd01b621c5266fc5f90738ad5-51954a350622f0323586.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-jsx-d24c22648ceb8a1acc75.js"/><link as="fetch" rel="preload" href="/page-data/blog/pay-attention/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/63159454.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="relative"><svg class="hidden fixed transform right-0 top-5 z-0 xl:block" width="404" height="784" fill="none" viewBox="0 0 404 784"><defs><pattern id="5d0dd344-b041-4d26-bec4-8d33ea57ec9b" x="0" y="0" width="20" height="20" patternUnits="userSpaceOnUse"><rect x="0" y="0" width="4" height="4" class="text-gray-200" fill="#edf2f7"></rect></pattern></defs><rect width="404" height="784" fill="url(#5d0dd344-b041-4d26-bec4-8d33ea57ec9b)"></rect></svg><div class="p-8 relative max-w-screen-xl xs:p-24"><div class="block mb-6 md:flex"><div class="w-full max-w-150"><a href="/"><img class="rounded-full transform transition-all duration-150 hover:scale-105" src="/static/me-1f578bfaf0d3f7902c20cba17a21c584.jpg" alt="Andre Lin"/></a></div><div class="flex-none pt-6 md:pt-1 md:flex-1 md:pl-20"><h1 class="text-5xl text-gray-900 font-bold leading-tight hover:text-black"><a href="/">Andre Lin</a></h1><br/><p class="text-gray-600">An aspring Machine Learning engineer hoping to integrate ML deployment into software development.</p><ul class="mt-6 uppercase tracking-wider"><li class="inline list-none pr-4"><a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="mailto:andre_lin@u.nus.edu">Email</a></li><li class="inline list-none pr-4"><a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="https://www.linkedin.com/in/andre-linhk/">LinkedIn</a></li><li class="inline list-none pr-4"><a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="https://github.com/4ndrelim">GitHub</a></li><li class="inline list-none pr-4"><a class="inline-block py-2 font-semibold text-xs text-gray-600 hover:text-black" href="/blog">Blog</a></li></ul></div></div><h1 class="mt-16 text-4xl text-gray-900 font-bold">Nothing Fancy, Pay Attention</h1><p class="text-gray-600 font-light">Posted on <!-- -->December 9, 2023</p><div class="mt-16 blog-content"><p><span style="font-size:10px;">Edit: At the time of writing, Grant from 3Blue1Brown released a marvelous video offering a high-level intuition behind the transformer architecture.</span>
<br/>
<span style="font-size:10px;">His videos are without a doubt, top-notch. But if you’re more inclined for a short read than watch, this piece might just be for you.</span></p>
<p>-insert scary transformer pic-</p>
<h2>The Last Decade</h2>
<p>Rcurrent Neural Networks (RNNs) were among the first architectures to process sequential data. And until 2017, RNNs remained as the primary neural network architecture, with Long Short-Term Memory (LSTM) units and Grated Recurrent Units (GRUs) - which can be thought of as a simpler version of LSTMs - being the most prominent and effective variants. In the present, LSTMs are becoming increasing obsolete, particularly in many areas of Natural Language Processing (NLP).</p>
<details>
    <summary>Applications for LSTMs</summary>
    <span style="font-size:14px;">
        Calling LSTMs 'obsolete' might be slightly too harsh. There remain use cases for LSTMs:
        <ul>
            <li><strong>Resource Constraints:</strong> LSTMs are more suitable for environments with limited computational resources, such as mobile devices.</li>
            <li><strong>Real-time Processing:</strong> Ideal for applications where data is processed in real-time, such as streaming data.</li>
            <li><strong>Small Data Scenarios:</strong> Performance degrades with increasing in amount of training data; after all, LSTM mitigates, not eliminate, exploding/vanishing gradient descent problem.</li>
            <li><strong>Specific Types of Sequential Tasks:</strong> Such as time-series forecasting where long-term historical context is valuable.</li>
        </ul>
    </span>
</details>
<details>
    <summary>Understanding the LSTM Unit</summary>
    <span style="font-size:14px;">
        Understanding LSTM isn't the main focus of this piece. But below outlines my brief intuition behind the LSTM unit
    </span>
</details>
<p><br><br/>
But there is merit for aspiring machine learners to study (or at least grasp the idea) the evolution of architectures starting from RNNs. Understanding their limitations sheds light on the inspiration behind the next development. RNNs sought to retain past information to influence subsequent outputs. They were simplistic in their design, which made them popular but also inadequate. It suffers greatly from exploding/vanishing gradient descent problem, which meant it is ultimately limited in capturing long-term dependencies.</p>
<p>LSTMs came into the picture and the key innovation in LSTMs that helps manage gradient issues is their gated architecture, better regulating the flow of information through the network. However, in practice, LSTM units are still chained in succession, and will struggle with extremely long sequences. Processing data sequentially (each step must wait for the previous step to complete) inherently limits the parallelization within the training process. And because it is uni-direcitonal (information only flows one way), it is perhaps not suited in capturing nuances in natural language due to its limited ability in gathering context from both past and future simultaneously.</p>
<p>Thus, the advent of Transformers.</p>
<h2>The Transformer</h2>
<details>
  <summary><b>A Fundamental Appreciation</b></summary>
    <span style="font-size:14px;">
      <p>Before going any further, it would be greatly beneficial to have acquired an understanding of the vanilla (dense) neural networks. Appreciate that ultimately, neural networks devolve to <b>just a bunch of weights and biases</b>. These are trainable parameters encapsulated in what is commonly referred to as a 'black box' - takes in some input and (hopefully) churn the desired output. <b>The hope becomes tangible if the optimal values of weights of biases are found</b>. And how these values are set, depends on the training process.</p>
      <p>Of course, there are more transformations applied, such as activation functions to introduce non-linearity and batch normalization for stability. You can think of these as fancy accessories that can be swapped among various kinds. And I suppose it does make some sense why they are desired, after all, if all we do is to linearly apply weights and biases, the accumulation of all operations ultimately remains linear no matter how many layers of weights and biases. <br/>
      <b>What hope would we have that this could capture intricate relationships with no clear separation</b>?</p>
      *-insert vanilla NN with activation fn-*
      <p><b>Watch this <a href="https://www.youtube.com/watch?v=aircAruvnKk">series</a> by 3Blue1Brown on Deep Learning if you're foreign to all these</b>! Episodes 1-3 offer an enlightening and intuitive perspective of Deep Learning. Episode 4 goes through the calculus behind Gradient Descent, the technique behind the training phase, and might be slightly heavy. If you're math-averse (though frankly, it's the notation that confuses people rather than the underlying concepts), feel free to skip it. The first 3 episodes should sufficiently and satisfyingly equip you with a solid grasp.</p>
      <p>A short aside on the term 'black box'. Not entirely a big fan of the term though I empathise its common usage. Indeed, the wall of numbers in the layers of a neural network can seem intimidating, and we have no robust intuitive explanation why the input magically transforms into the output. But the key insight is to realise that with a suitable loss function, we can, with Gradient Descent, guide the model to tune its weights and biases based on how similar it is to the desired output. <b>How do we know that Gradient Descent will correctly tune the weights?</b> 
      <br/>
      Well, this is where Mathematics enter. We can, mathematically show that <b>Gradient Descent always converges to some local minima</b>. Some people may lament that a local minimum isn't the global minimum. <b>But one should question, is the global minimum even desired..?</b> Recall that optimizing the objective function (used in training) does not always translate to the lowest cost in actual production / testing. More often than not, there exists a 'good enough' range of values for the weights to achieve satisfactory performance on the actual cost function. So perhaps, the local minimum found might just fall within this good region, and in some cases, might actually perform better on actual live data than with the weights found from the global minimum.</p>
      *-insert objective fn vs cost fn-*
      <p>One last thing, most people confuse the relationship between Backpropagation and Gradient Descent. Gradient Descent leverages Backpropagation to efficiently compute derivatives (gradients) of the loss function with respect to each tunable weights in the network. Sure, you can opt for a different technique to compute gradients, but it likely won't be as efficient as Backpropagation (which only require 2 passes!).</p>
    </span>
</details>
<p><br><br/>
The Transformer was introduced in the paper titled <a href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a> by a team of researchers from Google Brain. This groundbreaking paper revolutionized the field of Natural Language Processing, giving rise to various Large Language Models (LLMs) with vast linguistic capabilities - powerful enough to convince some people that a dystopian era of sentient (lol..) robots is looming.</p>
<p>Our beloved ChatGPT too, leverages a variant of the original transformer architecture to answer at our beck and call, mostly silly questions.</p>
<details>
  <summary>Anyway, before we continue our exploration of the transformer architecture from bottom-up, i'd like to try to convince (if you aren't already) why it shouldn't be too surprising that an inanimate object can sound so much like a human:</summary>
  <span style="font-size:14px;">
    <i>The Gentlemen Bastards</i> is one of my favourite medieval-fantasy series. It incorporated humour so well in the midst of chaos and danger. <i>The First Law</i> is also another series I greatly enjoyed, the grim fantasy setting and pragmatic characters was something I found oddly amusing. The authors have a way with words, they brought the characters to life and got me real hooked on them. But it's not like the authors actually walked into my study, sat next to me, and read their work aloud with such intensity to convey every intended nuance. All I did was read a chunk of text.
  </span>
</details>
<p><br></br>
And turns out, the 2017 paper has cracked the code behind human speech. We can train models to chain the right words, with the right punctuation, the appropriate pauses, and whatnot, altogether to emulate human speech.</p>
<p>Let’s explore how it does it :)</p>
<h3>Word Embeddings</h3>
<p>Machines generally only handle numeric input. So, words must first be converted to some numeric form. A vector of numbers is a natural choice. One common method used in the past is to represent words through <a href="https://www.youtube.com/watch?v=G2iVj7WKDFk">one-hot encoding (OHE)</a>. Already, you can probably sense this isn’t a good choice. For one, the length of the vector has to be the size of your vocabulary, which clearly does not scale well.</p>
<details>
  <summary>Why?</summary>
  <span style="font-size:14px;">
    Adding 1 more unique word to the vocabulary means adding 1 more dimension to the representation!
  </span>
</details>
One-hot encoded vectors are extremely sparse (each word exists in its own dimension). And it doesn't quite capture semantic relationship between words that are 'closer' in meaning, for instance, male and female vs animal. 
<p>Thus, the need for word embeddings. Word embeddings are still vectors of numbers, but now, distance between the vectors matter. <b>Think of these vectors as giving coordinates in some high dimensional space (albeit not as high as OHE) where words with similar meanings tend to be closer to each other in that space</b>. This might sound very intuitive and almost trivial, but you might be surprised to learn that it wasn’t until the mid-2010s that word embeddings received widespread adoption.</p>
<p>The implication is that the model can now learn patterns or relationship among words apply it elsewhere. For instance, if the model learns the vector difference (helpful for visualisation, but god knows whether the model learns more fanciful operations instead) between the representations of <em>Man</em> and <em>Woman</em>, it could apply and learn <em>King</em> as to <em>Queen</em>. <br/>
As another example, consider <em>Pasta</em> as to <em>Italy</em>, and if I ask <em>Ramen</em> as to <em>??</em>, you would most likely suggest <em>Japan</em>. This vector space representation is precisely what we desire to meaningfully capture the words.</p>
<details>
  <summary>How?</summary>
  <span style="font-size:14px;">
    <p>There are popular and powerful techniques such as Word2Vec and GloVe that can capture word embeddings very well. If you're mostly dealing with normal english (and probably other languages) vocabulary, existing pre-trained GloVe and Word2Vec models should already be highly effective. After all, we don't expect the English Dictionary to change too often.</p>
    <p>If you're interested, Word2Vec employs a shallow neural network (SURPRISE SURPRISE..) architecture to learn word embeddings. GloVe obtained embeddings differently by calculating global co-occurrence matrix. Yes.. fancy terms, feel free to read up.</p>
  </span>
</details>
<h3>Positional Encodings</h3>
<h3>Tokenization</h3>
<p>I’ve been casually assuming the model knows how to parse a sentence into individual words, and very naturally such that the individual ‘words’ align with the words in our english language vocabulary. This isn’t always the case. A more appropriate terminology would be ‘tokens’. A token could be a word or little pieces of words that are commonly used. This offers greater flexibility. For instance, the model might find it more helpful to have <em>‘-ing’</em> as a token rather than have to separately capture <em>‘eating’</em> and <em>‘playing’</em> on top of <em>‘eat’</em> and <em>‘play’</em>.</p>
<p>In practice, we often use some popular tokenizer from a Library (e.g. NLTK) to parse the corpus of text. Embeddings are then computed from these tokens found. The collection of all tokens is your dictionary (aka vocabulary).</p>
<h3>The Secret Sauce: Pay Attention!</h3>
<h3>Trusty-old Softmax</h3>
<h2>Heart of the Transformer: Self-Attention</h2>
<h2>Different Architectures</h2>
<h3>Encoder-Decoder</h3>
<h3>Encoder-Only</h3>
<h3>Decoder-Only</h3>
<h2>Heck, they are all the same!</h2></div></div></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script>
  
  
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', 'ADD YOUR TRACKING ID HERE', 'auto', {});
      
      
      
      
      
      }</script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/blog/pay-attention/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-b51087faa551b97588ee.js"],"app":["/app-0a9070be329fd8f91afd.js"],"component---src-pages-404-jsx":["/component---src-pages-404-jsx-4dac9a9f41a258176553.js"],"component---src-pages-blog-jsx":["/component---src-pages-blog-jsx-864eb7577a76e38df2f1.js"],"component---src-pages-index-jsx":["/component---src-pages-index-jsx-1287e07c7bfd4605697b.js"],"component---src-templates-blog-post-jsx":["/component---src-templates-blog-post-jsx-d24c22648ceb8a1acc75.js"]};/*]]>*/</script><script src="/polyfill-b51087faa551b97588ee.js" nomodule=""></script><script src="/component---src-templates-blog-post-jsx-d24c22648ceb8a1acc75.js" async=""></script><script src="/500c2994e08cbb1cd01b621c5266fc5f90738ad5-51954a350622f0323586.js" async=""></script><script src="/commons-460f29ae88868401563f.js" async=""></script><script src="/app-0a9070be329fd8f91afd.js" async=""></script><script src="/framework-e2d419ac45d8ae41957a.js" async=""></script><script src="/styles-755093da0c07f4b49226.js" async=""></script><script src="/webpack-runtime-2d432607a065c60c4715.js" async=""></script></body></html>